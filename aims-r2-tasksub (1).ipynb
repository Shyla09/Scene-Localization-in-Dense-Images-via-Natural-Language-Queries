{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2598787,"sourceType":"datasetVersion","datasetId":1573501}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T17:59:47.758537Z","iopub.execute_input":"2025-08-15T17:59:47.758806Z","iopub.status.idle":"2025-08-15T18:01:06.663286Z","shell.execute_reply.started":"2025-08-15T17:59:47.758783Z","shell.execute_reply":"2025-08-15T18:01:06.662585Z"}},"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.3.1\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-c272ksla\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-c272ksla\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=378c8ddd9966799ba9d3cf3441c62d2941957da5567dfc9eda756ce70f33db40\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ltknlzzl/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nfrom datasets import load_dataset\nimport os\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:01:08.989327Z","iopub.execute_input":"2025-08-15T18:01:08.990006Z","iopub.status.idle":"2025-08-15T18:01:14.167888Z","shell.execute_reply.started":"2025-08-15T18:01:08.989971Z","shell.execute_reply":"2025-08-15T18:01:14.167144Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"The following code snippet prepares the RefCOCOg dataset\n1. It loads the dataset from Hugging Face, then  filters it to include only samples with images physically present on the local system.\n2. From these validated samples, it selects a random subset of 7000 entries,\n3. Saves this prepared subset to disk for quick access.","metadata":{}},{"cell_type":"code","source":"\n# Step 2: Load RefCOCOg HF dataset\nds = load_dataset(\"jxu124/refcocog\")\n\n# Step 3: Define image root and compatibility filter\nimage_root = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images\"\n\ndef image_exists(example):\n    rel_path = example['image_path']\n    if rel_path.startswith(\"coco/\"):\n        rel_path = rel_path[len(\"coco/\"):]\n    img_path = os.path.join(image_root, rel_path)\n    return os.path.exists(img_path)\n\n# Step 4: Filter for samples with locally available images\nprint(\"Filtering for present images—this may take a moment...\")\nvalid_samples = ds['train'].filter(image_exists)\n\nprint(f\"Found {len(valid_samples)} valid samples with matching images.\")\n\n# Step 5: Select a random subset (e.g., 1000 samples)\nnum_subset = 7000\nrandom_indices = random.sample(range(len(valid_samples)), num_subset)\nsubset_ds = valid_samples.select(random_indices)\n\nprint(f\"Subset ready: {len(subset_ds)} samples.\")\n\n# Step 6: (Optional) Save subset for later fast reloading\nsubset_ds.save_to_disk(\"refcocog_train_subset_3000\")\n\n# Your new train_dataset for the model\n# from your RefCOCOgClipDataset class (as in your previous notebook):\n# train_dataset = RefCOCOgClipDataset(subset_ds, image_root, augment=True)\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:01:17.142513Z","iopub.execute_input":"2025-08-15T18:01:17.142957Z","iopub.status.idle":"2025-08-15T18:03:30.361082Z","shell.execute_reply.started":"2025-08-15T18:01:17.142933Z","shell.execute_reply":"2025-08-15T18:03:30.360485Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34934cd72ac74da2ac5f501da864e0c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-4fe3e6340cfb69(…):   0%|          | 0.00/39.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2176b5c0d2a54744859ce59c1ca9c3e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-15168dfe7(…):   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22a99c755a045e1af02b06f8bd64dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001-2316f36b19cd7f7(…):   0%|          | 0.00/4.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc69ebeba284e2491780788388b6f47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/42226 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d56a7862e684c5cba8a9ae4ae78533f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2573 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a336f2260bf495595a6a8aca1f1227b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5023 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e462f0b819ed4ae88aad470370defff2"}},"metadata":{}},{"name":"stdout","text":"Filtering for present images—this may take a moment...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/42226 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510f01e23ed64af197f84713307ed515"}},"metadata":{}},{"name":"stdout","text":"Found 42226 valid samples with matching images.\nSubset ready: 7000 samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0755f372f8e34763aa8b89b0ff61626f"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Following code snippet verifies the local presence of image files referenced in a dataset subset. It iterates through each sample, constructs the full image path, and checks if the file exists, reporting any missing files to ensure data integrity before further processing.","metadata":{}},{"cell_type":"code","source":"# Assumptions:\n# - subset_ds: HuggingFace Dataset containing the 3,000 subset samples\n# - image_root: Your COCO images root directory\ndef check_correspondence(subset_ds, image_root):\n    all_present = True\n    missing_files = []\n    for i, example in enumerate(subset_ds):\n        rel_path = example['image_path']\n        if rel_path.startswith(\"coco/\"):\n            rel_path = rel_path[len(\"coco/\"):]\n        img_path = os.path.join(image_root, rel_path)\n        if not os.path.exists(img_path):\n            all_present = False\n            missing_files.append(img_path)\n            print(f\"Missing file for index {i}: {img_path}\")\n    print(f\"\\nChecked {len(subset_ds)} samples.\")\n    if all_present:\n        print(\"All image files for the subset are present locally.\")\n    else:\n        print(f\"{len(missing_files)} image files are missing in your local dataset.\")\n\n# Run the check on your current subset\ncheck_correspondence(subset_ds, image_root)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:03:32.989434Z","iopub.execute_input":"2025-08-15T18:03:32.989894Z","iopub.status.idle":"2025-08-15T18:03:41.437123Z","shell.execute_reply.started":"2025-08-15T18:03:32.989866Z","shell.execute_reply":"2025-08-15T18:03:41.436456Z"}},"outputs":[{"name":"stdout","text":"\nChecked 7000 samples.\nAll image files for the subset are present locally.\ndone\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Following code snippet filters the dataset splits for existing images, then randomly samples and saves subsets for training, validation, and testing, ensuring a balanced distribution and local availability of data for model development.","metadata":{}},{"cell_type":"code","source":"import random\n\n# Step 1: Filter for present images in each split\ntrain_valid_samples = ds['train'].filter(image_exists)\nval_valid_samples = ds['validation'].filter(image_exists)\ntest_valid_samples = ds['test'].filter(image_exists)\n\nnum_total = 7000\n\n# Step 2: Calculate subset sizes (adjust as needed)\nnum_train = int(num_total * 0.7)\nnum_val = int(num_total * 0.15)\nnum_test = num_total - num_train - num_val  # ensures sum = 3000\n\n# Step 3: Sample subsets from each split (do not exceed available samples)\nnum_train = min(num_train, len(train_valid_samples))\nnum_val = min(num_val, len(val_valid_samples))\nnum_test = min(num_test, len(test_valid_samples))\n\ntrain_indices = random.sample(range(len(train_valid_samples)), num_train)\nval_indices = random.sample(range(len(val_valid_samples)), num_val)\ntest_indices = random.sample(range(len(test_valid_samples)), num_test)\n\ntrain_subset_ds = train_valid_samples.select(train_indices)\nval_subset_ds = val_valid_samples.select(val_indices)\ntest_subset_ds = test_valid_samples.select(test_indices)\n\n# Step 4: Save subsets to disk\ntrain_subset_ds.save_to_disk(\"refcocog_train_subset\")\nval_subset_ds.save_to_disk(\"refcocog_val_subset\")\ntest_subset_ds.save_to_disk(\"refcocog_test_subset\")\n\nprint(f\"Train subset size: {len(train_subset_ds)}\")\nprint(f\"Val subset size: {len(val_subset_ds)}\")\nprint(f\"Test subset size: {len(test_subset_ds)}\")\nprint(\"Saved all subsets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:03:44.673854Z","iopub.execute_input":"2025-08-15T18:03:44.674185Z","iopub.status.idle":"2025-08-15T18:04:03.361387Z","shell.execute_reply.started":"2025-08-15T18:03:44.674153Z","shell.execute_reply":"2025-08-15T18:04:03.360834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2573 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e32f8fe8954e18b2948427a36bf19b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5023 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6faf1db3ef6426da62e4484477de863"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f95630450a6843cda8068148f317a9ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1050 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9f73fed9bc4e81a641d819c29cd6e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1050 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c960af2dff4b4a228ebbb1a01e5880c1"}},"metadata":{}},{"name":"stdout","text":"Train subset size: 4900\nVal subset size: 1050\nTest subset size: 1050\nSaved all subsets.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Following code defines RefCOCOgClipDataset, a custom PyTorch dataset class that prepares image-text pairs and bounding box annotations for a machine learning model. For each data example,\n1. it loads and preprocesses images using CLIP's transforms,\n2. tokenizes the associated text query,\n3. normalizes the bounding box coordinates, making the data ready for model input.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nimport torch\nimport clip\n\nclass RefCOCOgClipDataset(Dataset):\n    def __init__(self, hf_dataset, image_root_dir, clip_model_name=\"ViT-B/32\", device=\"cpu\", augment=False):\n        self.dataset = hf_dataset\n        self.image_root_dir = image_root_dir\n        self.augment = augment\n        self.device = device\n        self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device=device)\n        # No augmentation means only default preprocessing\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        rel_path = example['image_path']\n        if rel_path.startswith(\"coco/\"):\n            rel_path = rel_path[len(\"coco/\"):]\n        img_path = os.path.join(self.image_root_dir, rel_path)\n        image = Image.open(img_path).convert('RGB')\n        width, height = image.size\n\n        # Get the query text from captions or sentences fields\n        if 'captions' in example and example['captions']:\n            query = example['captions'][0]\n        elif 'sentences' in example and 'sent' in example['sentences']:\n            query = example['sentences']['sent']\n        else:\n            query = \"\"\n\n        clip_text = clip.tokenize([query], truncate=True).squeeze(0)\n\n        # Normalize bbox coordinates\n        x, y, w, h = example['bbox']\n        x_min = x / width\n        y_min = y / height\n        x_max = (x + w) / width\n        y_max = (y + h) / height\n        norm_bbox = torch.tensor([x_min, y_min, x_max, y_max], dtype=torch.float)\n\n        # Preprocess image using CLIP's preprocess function\n        clip_image = self.clip_preprocess(image)\n\n        return {\n            'clip_image': clip_image,\n            'clip_text': clip_text,\n            'query_text': query,\n            'bbox': norm_bbox,\n            'orig_size': (width, height),\n            'image_path': img_path\n        }\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:04:06.036420Z","iopub.execute_input":"2025-08-15T18:04:06.036917Z","iopub.status.idle":"2025-08-15T18:04:09.323103Z","shell.execute_reply.started":"2025-08-15T18:04:06.036889Z","shell.execute_reply":"2025-08-15T18:04:09.322333Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Following code snippet instantiates dataset objects for training, validation, and testing using the RefCOCOgClipDataset class, then creates corresponding data loaders to efficiently batch and shuffle data for model training and evaluation.","metadata":{}},{"cell_type":"code","source":"train_dataset = RefCOCOgClipDataset(train_subset_ds, image_root)\nval_dataset = RefCOCOgClipDataset(val_subset_ds, image_root)\ntest_dataset = RefCOCOgClipDataset(test_subset_ds, image_root)\n\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:04:10.936499Z","iopub.execute_input":"2025-08-15T18:04:10.936979Z","iopub.status.idle":"2025-08-15T18:04:37.763341Z","shell.execute_reply.started":"2025-08-15T18:04:10.936957Z","shell.execute_reply":"2025-08-15T18:04:37.762493Z"}},"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 338M/338M [00:13<00:00, 27.0MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Following code defines the CrossModalClipFusionModel class, which is the core neural network for scene localization. \n1. It integrates a frozen CLIP backbone for extracting image and text features,\n2. uses linear projections and a Transformer Encoder for cross-modal fusion\n3. employs a feed-forward network to predict bounding box coordinates.\n4. The snippet then instantiates this model, preparing it for training or inference.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport clip\n\nclass CrossModalClipFusionModel(nn.Module):\n    def __init__(self, clip_model_name=\"ViT-B/32\", fusion_hidden_dim=512, transformer_layers=2, nhead=8, device=\"cuda\"):\n        super().__init__()\n        self.device = device\n        self.clip_model, _ = clip.load(clip_model_name, device=device)\n        for param in self.clip_model.parameters():\n            param.requires_grad = False  # freeze CLIP backbone\n\n        self.visual_proj = nn.Linear(512, fusion_hidden_dim)\n        self.text_proj = nn.Linear(512, fusion_hidden_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=fusion_hidden_dim, nhead=nhead)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n\n        self.bbox_head = nn.Sequential(\n            nn.Linear(fusion_hidden_dim, fusion_hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(fusion_hidden_dim // 2, 4)\n        )\n\n    def forward(self, clip_image, clip_text):\n        image_feat = self.clip_model.encode_image(clip_image).float()\n        text_feat = self.clip_model.encode_text(clip_text).float()\n        v_proj = self.visual_proj(image_feat)\n        t_proj = self.text_proj(text_feat)\n        fuse = torch.stack([v_proj, t_proj], dim=0)\n        fuse = self.transformer_encoder(fuse)\n        joint_emb = fuse.mean(dim=0)\n        bbox_pred = self.bbox_head(joint_emb)\n        bbox_pred = torch.sigmoid(bbox_pred)  # normalized bbox 0~1\n        return bbox_pred\n\n# Instantiate model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CrossModalClipFusionModel(device=device).to(device)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:04:40.015990Z","iopub.execute_input":"2025-08-15T18:04:40.016306Z","iopub.status.idle":"2025-08-15T18:04:44.760220Z","shell.execute_reply.started":"2025-08-15T18:04:40.016283Z","shell.execute_reply":"2025-08-15T18:04:44.759309Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Following code defines a custom loss function for bounding box regression, combining two key components. \n1. It includes bbox_iou, a utility function to calculate the Intersection over Union (IoU) between predicted and ground truth bounding boxes.\n2. The main bbox_mixed_loss then combines Smooth L1 Loss (for standard regression accuracy) with a negative log IoU loss (to directly optimize for better overlap), providing a robust metric for training object localization models.","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bbox_iou(box1, box2, eps=1e-6):\n    x1 = torch.max(box1[..., 0], box2[..., 0])\n    y1 = torch.max(box1[..., 1], box2[..., 1])\n    x2 = torch.min(box1[..., 2], box2[..., 2])\n    y2 = torch.min(box1[..., 3], box2[..., 3])\n    inter_area = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n    box1_area = (box1[..., 2] - box1[..., 0]).clamp(min=0) * (box1[..., 3] - box1[..., 1]).clamp(min=0)\n    box2_area = (box2[..., 2] - box2[..., 0]).clamp(min=0) * (box2[..., 3] - box2[..., 1]).clamp(min=0)\n    union_area = box1_area + box2_area - inter_area + eps\n    return inter_area / union_area\n\ndef bbox_mixed_loss(pred_bboxes, gt_bboxes, alpha=1.0, beta=2.0):\n    reg_loss = F.smooth_l1_loss(pred_bboxes, gt_bboxes)\n    ious = bbox_iou(pred_bboxes, gt_bboxes)\n    iou_loss = -torch.log(ious + 1e-6).mean()\n    return alpha * reg_loss + beta * iou_loss\ncriterion = bbox_mixed_loss\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:04:49.756968Z","iopub.execute_input":"2025-08-15T18:04:49.757566Z","iopub.status.idle":"2025-08-15T18:04:49.764775Z","shell.execute_reply.started":"2025-08-15T18:04:49.757542Z","shell.execute_reply":"2025-08-15T18:04:49.764050Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Following code snippet implements the main training loop for the neural network. \n1. It initializes an AdamW optimizer and a learning rate scheduler, then\n2. iteratively trains the model for a set number of epochs.\n3. Within each epoch, it calculates training and validation losses, performs backpropagation and optimization, and incorporates early stopping to save the best-performing model based on validation loss, ensuring efficient and effective training.","metadata":{}},{"cell_type":"code","source":"import torch\nimport copy\nfrom tqdm import tqdm\n\n# --- ADVANCED OPTIMIZER ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)  # typical wd for AdamW\n\n# --- LEARNING RATE SCHEDULER ---\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n)\n\nnum_epochs = 10  # max epochs\npatience = 3     # for early stopping\nbest_val_loss = float('inf')\nepochs_no_improve = 0\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    for batch_idx, batch in enumerate(loop):\n        clip_images = batch['clip_image'].to(device)\n        clip_texts = batch['clip_text'].to(device)\n        gt_bboxes  = batch['bbox'].to(device)\n\n        optimizer.zero_grad()\n        pred_bboxes = model(clip_images, clip_texts)\n        loss = criterion(pred_bboxes, gt_bboxes)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        avg_loss_so_far = total_loss / (batch_idx + 1)\n        loop.set_postfix(batch_loss=loss.item(), avg_loss=avg_loss_so_far)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_loss_so_far:.4f}\")\n\n    # Validation loss computation\n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            clip_images = batch['clip_image'].to(device)\n            clip_texts = batch['clip_text'].to(device)\n            gt_bboxes  = batch['bbox'].to(device)\n            pred_bboxes = model(clip_images, clip_texts)\n            val_loss = criterion(pred_bboxes, gt_bboxes)\n            val_losses.append(val_loss.item())\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}\")\n\n    scheduler.step(avg_val_loss)  # --- STEP THE SCHEDULER ---\n\n    # Early stopping check\n    if avg_val_loss < best_val_loss - 1e-4:  # minimal improvement threshold\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        best_model_wts = copy.deepcopy(model.state_dict())\n        print(f\"Validation loss improved, saving best model at epoch {epoch+1}.\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# Load best model weights after training\nmodel.load_state_dict(best_model_wts)\nprint(\"Training complete. Best model loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:04:52.236223Z","iopub.execute_input":"2025-08-15T18:04:52.236915Z","iopub.status.idle":"2025-08-15T18:09:33.275552Z","shell.execute_reply.started":"2025-08-15T18:04:52.236891Z","shell.execute_reply":"2025-08-15T18:09:33.274744Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\nEpoch 1/10: 100%|██████████| 307/307 [00:48<00:00,  6.38it/s, avg_loss=3.21, batch_loss=2.03]","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Training Loss: 3.2111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Validation Loss: 2.9788\nValidation loss improved, saving best model at epoch 1.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 307/307 [00:30<00:00, 10.18it/s, avg_loss=2.95, batch_loss=2.36]","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] Training Loss: 2.9516\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] Validation Loss: 2.9398\nValidation loss improved, saving best model at epoch 2.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 307/307 [00:30<00:00, 10.00it/s, avg_loss=2.91, batch_loss=3.82]","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] Training Loss: 2.9060\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] Validation Loss: 2.9628\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 307/307 [00:31<00:00,  9.77it/s, avg_loss=2.86, batch_loss=2.33]","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] Training Loss: 2.8613\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] Validation Loss: 2.9168\nValidation loss improved, saving best model at epoch 4.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 307/307 [00:30<00:00, 10.06it/s, avg_loss=2.86, batch_loss=2.83]","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] Training Loss: 2.8602\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] Validation Loss: 2.9328\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 307/307 [00:30<00:00, 10.01it/s, avg_loss=2.87, batch_loss=3.6] ","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] Training Loss: 2.8727\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] Validation Loss: 3.0420\nNo improvement in validation loss for 2 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 307/307 [00:30<00:00, 10.18it/s, avg_loss=2.9, batch_loss=2.8]  ","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] Training Loss: 2.8953\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] Validation Loss: 2.9410\nNo improvement in validation loss for 3 epochs.\nEarly stopping triggered.\nTraining complete. Best model loaded.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Following code snippet handles saving and loading the trained model's weights.","metadata":{}},{"cell_type":"code","source":"# Save the best model weights to disk\ntorch.save(model.state_dict(), \"best_model.pt\")\nprint(\"Saved best model weights to 'best_model.pt'\")\n# Load the best model weights from disk\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\nprint(\"Best model weights loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:12:17.572694Z","iopub.execute_input":"2025-08-15T18:12:17.573502Z","iopub.status.idle":"2025-08-15T18:12:18.383150Z","shell.execute_reply.started":"2025-08-15T18:12:17.573471Z","shell.execute_reply":"2025-08-15T18:12:18.382290Z"}},"outputs":[{"name":"stdout","text":"Saved best model weights to 'best_model.pt'\nBest model weights loaded.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"This code snippet provides utility functions for evaluating object detection models. \nIt includes \n1. safe_unpack_orig_size for safely extracting image dimensions,\n2. denormalize_bbox to convert normalized bounding box predictions back to original image coordinates,\n3. box_iou to calculate the Intersection over Union metric.\n4. The main evaluate_model_iou function then uses these utilities to compute the mean IoU of a model's predictions against ground truth bounding boxes over a given dataset.","metadata":{}},{"cell_type":"code","source":"def safe_unpack_orig_size(orig_size):\n    import numpy as np\n    if isinstance(orig_size, torch.Tensor):\n        # Convert tensor to numpy array and flatten\n        orig_size = orig_size.detach().cpu().numpy().flatten()\n    \n    # Check if the array has at least two elements\n    if len(orig_size) < 2:\n        return 0, 0\n    \n    return int(orig_size[0]), int(orig_size[1])\n\ndef denormalize_bbox(bbox, width, height):\n    import numpy as np\n    if isinstance(bbox, torch.Tensor):\n        bbox = bbox.detach().cpu().numpy()\n    bbox = np.array(bbox).flatten()[:4]\n    return [\n        float(bbox[0]) * width,\n        float(bbox[1]) * height,\n        float(bbox[2]) * width,\n        float(bbox[3]) * height\n    ]\n\ndef box_iou(box1, box2):\n    x1 = float(max(box1[0], box2[0]))\n    y1 = float(max(box1[1], box2[1]))\n    x2 = float(min(box1[2], box2[2]))\n    y2 = float(min(box1[3], box2[3]))\n    inter_w = max(0., x2 - x1)\n    inter_h = max(0., y2 - y1)\n    inter_area = inter_w * inter_h\n    box1_area = max(0., box1[2] - box1[0]) * max(0., box1[3] - box1[1])\n    box2_area = max(0., box2[2] - box2[0]) * max(0., box2[3] - box2[1])\n    union_area = box1_area + box2_area - inter_area + 1e-6\n    if union_area == 0.:\n        return 0.\n    return inter_area / union_area\n\ndef evaluate_model_iou(model, data_loader, device, max_batches=None):\n    model.eval()\n    all_ious = []\n    import numpy as np\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(data_loader):\n            clip_images = batch['clip_image'].to(device)\n            clip_texts = batch['clip_text'].to(device)\n            pred_bboxes = model(clip_images, clip_texts)\n            gt_bboxes = batch['bbox']\n            orig_sizes = batch['orig_size']\n            \n            # Use zip for safe iteration over batch elements\n            for pred_box_norm, gt_box_norm, orig_size_item in zip(pred_bboxes, gt_bboxes, orig_sizes):\n                width, height = safe_unpack_orig_size(orig_size_item)\n                \n                # Denormalize bounding boxes\n                pred_box_list = denormalize_bbox(pred_box_norm, width, height)\n                gt_box_list = denormalize_bbox(gt_box_norm, width, height)\n                \n                # Calculate IoU and append\n                iou = box_iou(pred_box_list, gt_box_list)\n                all_ious.append(iou)\n\n            if max_batches is not None and batch_idx + 1 >= max_batches:\n                break\n    \n    mean_iou = np.mean(all_ious) if all_ious else 0.0\n    print(f\"Mean IoU: {mean_iou:.4f} on {len(all_ious)} samples\")\n    return mean_iou\n\n# Example usage to demonstrate a successful run:\n# # Load best model weights before evaluation\n# model.load_state_dict(torch.load(\"best_model.pt\"))\n# model.eval()\n#\n# # Evaluate on validation set\n# val_mean_iou = evaluate_model_iou(model, val_loader, device)\n# print(\"Validation Mean IoU:\", val_mean_iou)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:12:24.171747Z","iopub.execute_input":"2025-08-15T18:12:24.172056Z","iopub.status.idle":"2025-08-15T18:12:24.183755Z","shell.execute_reply.started":"2025-08-15T18:12:24.172001Z","shell.execute_reply":"2025-08-15T18:12:24.183056Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Following code snippet evaluates the trained model's performance on both validation and test datasets. It calculates and prints the Mean IoU (Intersection over Union) for both the validation and test sets, providing a quantitative assessment of the model's localization accuracy.","metadata":{}},{"cell_type":"code","source":"# Load best model weights before evaluation\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\n# Evaluate on validation set\nval_mean_iou = evaluate_model_iou(model, val_loader, device)\nprint(\"Validation Mean IoU:\", val_mean_iou)\n\n# Evaluate on test set\ntest_mean_iou = evaluate_model_iou(model, test_loader, device)\nprint(\"Test Mean IoU:\", test_mean_iou)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:53:49.892531Z","iopub.execute_input":"2025-08-15T16:53:49.893032Z","iopub.status.idle":"2025-08-15T16:54:05.427839Z","shell.execute_reply.started":"2025-08-15T16:53:49.893007Z","shell.execute_reply":"2025-08-15T16:54:05.427047Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.2896 on 132 samples\nValidation Mean IoU: 0.289642835112249\nMean IoU: 0.2981 on 132 samples\nTest Mean IoU: 0.2981048999688267\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Following code defines a function visualize_prediction_and_crop that takes a trained model, an image path, and a text query to visualize the model's object localization. \n1. It loads and preprocesses the image and text,\n2. uses the model to predict a bounding box,\n3. denormalizes the coordinates,\n4. draws the box on the original image,\n5. crops the region.\n6. The snippet then demonstrates its usage by applying the function to a specific image and query, saving the results as PNG files.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport clip\n\ndef visualize_prediction_and_crop(model, clip_preprocess, device, image_path, text_query):\n    \"\"\"\n    Takes a model, image path, and text query, and returns the cropped image\n    along with a visualization of the predicted bounding box.\n\n    Args:\n        model: The trained CrossModalClipFusionModel.\n        clip_preprocess: The CLIP image preprocessing function.\n        device: The device to run the model on ('cuda' or 'cpu').\n        image_path (str): The path to the input image.\n        text_query (str): The text description of the object to find.\n\n    Returns:\n        image_with_box (PIL.Image): The original image with the bounding box drawn.\n        cropped_image (PIL.Image): The cropped region of the image.\n    \"\"\"\n    # 1. Load and preprocess the image and text\n    image = Image.open(image_path).convert('RGB')\n    width, height = image.size\n    \n    clip_image = clip_preprocess(image).unsqueeze(0).to(device)\n    clip_text = clip.tokenize([text_query], truncate=True).to(device)\n\n    # 2. Get the model's bounding box prediction\n    model.eval()\n    with torch.no_grad():\n        pred_bbox_norm = model(clip_image, clip_text)\n    \n    # 3. Denormalize the predicted bounding box\n    x_min_norm, y_min_norm, x_max_norm, y_max_norm = pred_bbox_norm[0].cpu().numpy()\n    \n    x_min = int(x_min_norm * width)\n    y_min = int(y_min_norm * height)\n    x_max = int(x_max_norm * width)\n    y_max = int(y_max_norm * height)\n    \n    # Ensure coordinates are within image bounds\n    x_min = max(0, x_min)\n    y_min = max(0, y_min)\n    x_max = min(width, x_max)\n    y_max = min(height, y_max)\n\n    print(\"Predicted Bounding Box (x_min, y_min, x_max, y_max):\", (x_min, y_min, x_max, y_max))\n\n    # 4. Create a copy to draw on and the cropped image\n    image_with_box = image.copy()\n    draw = ImageDraw.Draw(image_with_box)\n    draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n    \n    # 5. Crop the image based on the predicted bounding box\n    cropped_image = image.crop((x_min, y_min, x_max, y_max))\n\n    return image_with_box, cropped_image\n\n# Assume 'model', 'clip_preprocess', and 'device' are already defined from previous steps\n# The following code block will run the test on the specified image.\n\n# Image path and text query\nimage_path = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014/COCO_test2014_000000000016.jpg\"\ntext_query = \"a baseball player swinging a bat\"\n\n# Run the function\ntry:\n    image_with_box, cropped_region = visualize_prediction_and_crop(\n        model=model,\n        clip_preprocess=test_dataset.clip_preprocess,\n        device=device,\n        image_path=image_path,\n        text_query=text_query\n    )\n    \n    # Display the results\n    image_with_box.save(\"prediction_visualization.png\")\n    cropped_region.save(\"cropped_region.png\")\n    print(\"Saved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport clip\n\ndef visualize_prediction_and_crop(model, clip_preprocess, device, image_path, text_query):\n    \"\"\"\n    Takes a model, image path, and text query, and returns the cropped image\n    along with a visualization of the predicted bounding box.\n\n    Args:\n        model: The trained CrossModalClipFusionModel.\n        clip_preprocess: The CLIP image preprocessing function.\n        device: The device to run the model on ('cuda' or 'cpu').\n        image_path (str): The path to the input image.\n        text_query (str): The text description of the object to find.\n\n    Returns:\n        image_with_box (PIL.Image): The original image with the bounding box drawn.\n        cropped_image (PIL.Image): The cropped region of the image.\n    \"\"\"\n    # 1. Load and preprocess the image and text\n    image = Image.open(image_path).convert('RGB')\n    width, height = image.size\n    \n    clip_image = clip_preprocess(image).unsqueeze(0).to(device)\n    clip_text = clip.tokenize([text_query], truncate=True).to(device)\n\n    # 2. Get the model's bounding box prediction\n    model.eval()\n    with torch.no_grad():\n        pred_bbox_norm = model(clip_image, clip_text)\n    \n    # 3. Denormalize the predicted bounding box\n    x_min_norm, y_min_norm, x_max_norm, y_max_norm = pred_bbox_norm[0].cpu().numpy()\n    \n    x_min = int(x_min_norm * width)\n    y_min = int(y_min_norm * height)\n    x_max = int(x_max_norm * width)\n    y_max = int(y_max_norm * height)\n    \n    # Ensure coordinates are within image bounds\n    x_min = max(0, x_min)\n    y_min = max(0, y_min)\n    x_max = min(width, x_max)\n    y_max = min(height, y_max)\n\n    print(\"Predicted Bounding Box (x_min, y_min, x_max, y_max):\", (x_min, y_min, x_max, y_max))\n\n    # 4. Create a copy to draw on and the cropped image\n    image_with_box = image.copy()\n    draw = ImageDraw.Draw(image_with_box)\n    draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n    \n    # 5. Crop the image based on the predicted bounding box\n    cropped_image = image.crop((x_min, y_min, x_max, y_max))\n\n    return image_with_box, cropped_image\n\n# Assume 'model', 'clip_preprocess', and 'device' are already defined from previous steps\n# The following code block will run the test on the specified image.\n\n# Image path and text query\nimage_path = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014/COCO_test2014_000000000069.jpg\"\ntext_query = \"clown dancing\"\n\n# Run the function\ntry:\n    image_with_box, cropped_region = visualize_prediction_and_crop(\n        model=model,\n        clip_preprocess=test_dataset.clip_preprocess,\n        device=device,\n        image_path=image_path,\n        text_query=text_query\n    )\n    \n    # Display the results\n    image_with_box.save(\"prediction_visualization.png\")\n    cropped_region.save(\"cropped_region.png\")\n    print(\"Saved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:13:00.823275Z","iopub.execute_input":"2025-08-15T18:13:00.823567Z","iopub.status.idle":"2025-08-15T18:13:01.045496Z","shell.execute_reply.started":"2025-08-15T18:13:00.823546Z","shell.execute_reply":"2025-08-15T18:13:01.044860Z"}},"outputs":[{"name":"stdout","text":"Predicted Bounding Box (x_min, y_min, x_max, y_max): (24, 56, 639, 428)\nSaved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\n","output_type":"stream"}],"execution_count":16}]}