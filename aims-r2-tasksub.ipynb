{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2598787,"sourceType":"datasetVersion","datasetId":1573501}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:28:34.242389Z","iopub.execute_input":"2025-08-15T16:28:34.242672Z","iopub.status.idle":"2025-08-15T16:29:47.653585Z","shell.execute_reply.started":"2025-08-15T16:28:34.242650Z","shell.execute_reply":"2025-08-15T16:29:47.652895Z"}},"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.3.1\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jq8f31dm\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jq8f31dm\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=538ad724d96a72db07a09463fbd14aed6aca19735985701de2dee0cf680d25f3\n  Stored in directory: /tmp/pip-ephem-wheel-cache-otygfvlx/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nfrom datasets import load_dataset\nimport os\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:06.360968Z","iopub.execute_input":"2025-08-15T16:46:06.361268Z","iopub.status.idle":"2025-08-15T16:46:06.365411Z","shell.execute_reply.started":"2025-08-15T16:46:06.361247Z","shell.execute_reply":"2025-08-15T16:46:06.364636Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\n# Step 2: Load RefCOCOg HF dataset\nds = load_dataset(\"jxu124/refcocog\")\n\n# Step 3: Define image root and compatibility filter\nimage_root = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images\"\n\ndef image_exists(example):\n    rel_path = example['image_path']\n    if rel_path.startswith(\"coco/\"):\n        rel_path = rel_path[len(\"coco/\"):]\n    img_path = os.path.join(image_root, rel_path)\n    return os.path.exists(img_path)\n\n# Step 4: Filter for samples with locally available images\nprint(\"Filtering for present images—this may take a moment...\")\nvalid_samples = ds['train'].filter(image_exists)\n\nprint(f\"Found {len(valid_samples)} valid samples with matching images.\")\n\n# Step 5: Select a random subset (e.g., 1000 samples)\nnum_subset = 7000\nrandom_indices = random.sample(range(len(valid_samples)), num_subset)\nsubset_ds = valid_samples.select(random_indices)\n\nprint(f\"Subset ready: {len(subset_ds)} samples.\")\n\n# Step 6: (Optional) Save subset for later fast reloading\nsubset_ds.save_to_disk(\"refcocog_train_subset_3000\")\n\n# Your new train_dataset for the model\n# from your RefCOCOgClipDataset class (as in your previous notebook):\n# train_dataset = RefCOCOgClipDataset(subset_ds, image_root, augment=True)\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:09.716574Z","iopub.execute_input":"2025-08-15T16:46:09.717178Z","iopub.status.idle":"2025-08-15T16:46:11.249355Z","shell.execute_reply.started":"2025-08-15T16:46:09.717157Z","shell.execute_reply":"2025-08-15T16:46:11.248760Z"}},"outputs":[{"name":"stdout","text":"Filtering for present images—this may take a moment...\nFound 42226 valid samples with matching images.\nSubset ready: 7000 samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be8a01aa981f48d5b86464862a5462f3"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Assumptions:\n# - subset_ds: HuggingFace Dataset containing the 3,000 subset samples\n# - image_root: Your COCO images root directory\ndef check_correspondence(subset_ds, image_root):\n    all_present = True\n    missing_files = []\n    for i, example in enumerate(subset_ds):\n        rel_path = example['image_path']\n        if rel_path.startswith(\"coco/\"):\n            rel_path = rel_path[len(\"coco/\"):]\n        img_path = os.path.join(image_root, rel_path)\n        if not os.path.exists(img_path):\n            all_present = False\n            missing_files.append(img_path)\n            print(f\"Missing file for index {i}: {img_path}\")\n    print(f\"\\nChecked {len(subset_ds)} samples.\")\n    if all_present:\n        print(\"All image files for the subset are present locally.\")\n    else:\n        print(f\"{len(missing_files)} image files are missing in your local dataset.\")\n\n# Run the check on your current subset\ncheck_correspondence(subset_ds, image_root)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:16.305759Z","iopub.execute_input":"2025-08-15T16:46:16.306032Z","iopub.status.idle":"2025-08-15T16:46:24.838436Z","shell.execute_reply.started":"2025-08-15T16:46:16.306014Z","shell.execute_reply":"2025-08-15T16:46:24.837663Z"}},"outputs":[{"name":"stdout","text":"\nChecked 7000 samples.\nAll image files for the subset are present locally.\ndone\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import random\n\n# Step 1: Filter for present images in each split\ntrain_valid_samples = ds['train'].filter(image_exists)\nval_valid_samples = ds['validation'].filter(image_exists)\ntest_valid_samples = ds['test'].filter(image_exists)\n\nnum_total = 7000\n\n# Step 2: Calculate subset sizes (adjust as needed)\nnum_train = int(num_total * 0.7)\nnum_val = int(num_total * 0.15)\nnum_test = num_total - num_train - num_val  # ensures sum = 3000\n\n# Step 3: Sample subsets from each split (do not exceed available samples)\nnum_train = min(num_train, len(train_valid_samples))\nnum_val = min(num_val, len(val_valid_samples))\nnum_test = min(num_test, len(test_valid_samples))\n\ntrain_indices = random.sample(range(len(train_valid_samples)), num_train)\nval_indices = random.sample(range(len(val_valid_samples)), num_val)\ntest_indices = random.sample(range(len(test_valid_samples)), num_test)\n\ntrain_subset_ds = train_valid_samples.select(train_indices)\nval_subset_ds = val_valid_samples.select(val_indices)\ntest_subset_ds = test_valid_samples.select(test_indices)\n\n# Step 4: Save subsets to disk\ntrain_subset_ds.save_to_disk(\"refcocog_train_subset\")\nval_subset_ds.save_to_disk(\"refcocog_val_subset\")\ntest_subset_ds.save_to_disk(\"refcocog_test_subset\")\n\nprint(f\"Train subset size: {len(train_subset_ds)}\")\nprint(f\"Val subset size: {len(val_subset_ds)}\")\nprint(f\"Test subset size: {len(test_subset_ds)}\")\nprint(\"Saved all subsets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:31.175154Z","iopub.execute_input":"2025-08-15T16:46:31.175436Z","iopub.status.idle":"2025-08-15T16:46:31.459186Z","shell.execute_reply.started":"2025-08-15T16:46:31.175415Z","shell.execute_reply":"2025-08-15T16:46:31.458602Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85415f948baf4dd1abf7021ba9921cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1050 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c00578e3e549bebb6b109f179363ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1050 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f9ccb965aa40aaa46a636dfe95bd0d"}},"metadata":{}},{"name":"stdout","text":"Train subset size: 4900\nVal subset size: 1050\nTest subset size: 1050\nSaved all subsets.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nimport torch\nimport clip\n\nclass RefCOCOgClipDataset(Dataset):\n    def __init__(self, hf_dataset, image_root_dir, clip_model_name=\"ViT-B/32\", device=\"cpu\", augment=False):\n        self.dataset = hf_dataset\n        self.image_root_dir = image_root_dir\n        self.augment = augment\n        self.device = device\n        self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device=device)\n        # No augmentation means only default preprocessing\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        rel_path = example['image_path']\n        if rel_path.startswith(\"coco/\"):\n            rel_path = rel_path[len(\"coco/\"):]\n        img_path = os.path.join(self.image_root_dir, rel_path)\n        image = Image.open(img_path).convert('RGB')\n        width, height = image.size\n\n        # Get the query text from captions or sentences fields\n        if 'captions' in example and example['captions']:\n            query = example['captions'][0]\n        elif 'sentences' in example and 'sent' in example['sentences']:\n            query = example['sentences']['sent']\n        else:\n            query = \"\"\n\n        clip_text = clip.tokenize([query], truncate=True).squeeze(0)\n\n        # Normalize bbox coordinates\n        x, y, w, h = example['bbox']\n        x_min = x / width\n        y_min = y / height\n        x_max = (x + w) / width\n        y_max = (y + h) / height\n        norm_bbox = torch.tensor([x_min, y_min, x_max, y_max], dtype=torch.float)\n\n        # Preprocess image using CLIP's preprocess function\n        clip_image = self.clip_preprocess(image)\n\n        return {\n            'clip_image': clip_image,\n            'clip_text': clip_text,\n            'query_text': query,\n            'bbox': norm_bbox,\n            'orig_size': (width, height),\n            'image_path': img_path\n        }\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:38.711625Z","iopub.execute_input":"2025-08-15T16:46:38.712250Z","iopub.status.idle":"2025-08-15T16:46:38.720926Z","shell.execute_reply.started":"2025-08-15T16:46:38.712226Z","shell.execute_reply":"2025-08-15T16:46:38.720199Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"train_dataset = RefCOCOgClipDataset(train_subset_ds, image_root)\nval_dataset = RefCOCOgClipDataset(val_subset_ds, image_root)\ntest_dataset = RefCOCOgClipDataset(test_subset_ds, image_root)\n\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:46:44.366310Z","iopub.execute_input":"2025-08-15T16:46:44.366789Z","iopub.status.idle":"2025-08-15T16:46:57.271966Z","shell.execute_reply.started":"2025-08-15T16:46:44.366763Z","shell.execute_reply":"2025-08-15T16:46:57.271216Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport clip\n\nclass CrossModalClipFusionModel(nn.Module):\n    def __init__(self, clip_model_name=\"ViT-B/32\", fusion_hidden_dim=512, transformer_layers=2, nhead=8, device=\"cuda\"):\n        super().__init__()\n        self.device = device\n        self.clip_model, _ = clip.load(clip_model_name, device=device)\n        for param in self.clip_model.parameters():\n            param.requires_grad = False  # freeze CLIP backbone\n\n        self.visual_proj = nn.Linear(512, fusion_hidden_dim)\n        self.text_proj = nn.Linear(512, fusion_hidden_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=fusion_hidden_dim, nhead=nhead)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n\n        self.bbox_head = nn.Sequential(\n            nn.Linear(fusion_hidden_dim, fusion_hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(fusion_hidden_dim // 2, 4)\n        )\n\n    def forward(self, clip_image, clip_text):\n        image_feat = self.clip_model.encode_image(clip_image).float()\n        text_feat = self.clip_model.encode_text(clip_text).float()\n        v_proj = self.visual_proj(image_feat)\n        t_proj = self.text_proj(text_feat)\n        fuse = torch.stack([v_proj, t_proj], dim=0)\n        fuse = self.transformer_encoder(fuse)\n        joint_emb = fuse.mean(dim=0)\n        bbox_pred = self.bbox_head(joint_emb)\n        bbox_pred = torch.sigmoid(bbox_pred)  # normalized bbox 0~1\n        return bbox_pred\n\n# Instantiate model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CrossModalClipFusionModel(device=device).to(device)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:47:01.878353Z","iopub.execute_input":"2025-08-15T16:47:01.878900Z","iopub.status.idle":"2025-08-15T16:47:06.101686Z","shell.execute_reply.started":"2025-08-15T16:47:01.878877Z","shell.execute_reply":"2025-08-15T16:47:06.100966Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bbox_iou(box1, box2, eps=1e-6):\n    x1 = torch.max(box1[..., 0], box2[..., 0])\n    y1 = torch.max(box1[..., 1], box2[..., 1])\n    x2 = torch.min(box1[..., 2], box2[..., 2])\n    y2 = torch.min(box1[..., 3], box2[..., 3])\n    inter_area = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n    box1_area = (box1[..., 2] - box1[..., 0]).clamp(min=0) * (box1[..., 3] - box1[..., 1]).clamp(min=0)\n    box2_area = (box2[..., 2] - box2[..., 0]).clamp(min=0) * (box2[..., 3] - box2[..., 1]).clamp(min=0)\n    union_area = box1_area + box2_area - inter_area + eps\n    return inter_area / union_area\n\ndef bbox_mixed_loss(pred_bboxes, gt_bboxes, alpha=1.0, beta=2.0):\n    reg_loss = F.smooth_l1_loss(pred_bboxes, gt_bboxes)\n    ious = bbox_iou(pred_bboxes, gt_bboxes)\n    iou_loss = -torch.log(ious + 1e-6).mean()\n    return alpha * reg_loss + beta * iou_loss\ncriterion = bbox_mixed_loss\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:47:10.120279Z","iopub.execute_input":"2025-08-15T16:47:10.121002Z","iopub.status.idle":"2025-08-15T16:47:10.128035Z","shell.execute_reply.started":"2025-08-15T16:47:10.120977Z","shell.execute_reply":"2025-08-15T16:47:10.127266Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport copy\nfrom tqdm import tqdm\n\n# --- ADVANCED OPTIMIZER ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)  # typical wd for AdamW\n\n# --- LEARNING RATE SCHEDULER ---\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n)\n\nnum_epochs = 10  # max epochs\npatience = 3     # for early stopping\nbest_val_loss = float('inf')\nepochs_no_improve = 0\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    for batch_idx, batch in enumerate(loop):\n        clip_images = batch['clip_image'].to(device)\n        clip_texts = batch['clip_text'].to(device)\n        gt_bboxes  = batch['bbox'].to(device)\n\n        optimizer.zero_grad()\n        pred_bboxes = model(clip_images, clip_texts)\n        loss = criterion(pred_bboxes, gt_bboxes)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        avg_loss_so_far = total_loss / (batch_idx + 1)\n        loop.set_postfix(batch_loss=loss.item(), avg_loss=avg_loss_so_far)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_loss_so_far:.4f}\")\n\n    # Validation loss computation\n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            clip_images = batch['clip_image'].to(device)\n            clip_texts = batch['clip_text'].to(device)\n            gt_bboxes  = batch['bbox'].to(device)\n            pred_bboxes = model(clip_images, clip_texts)\n            val_loss = criterion(pred_bboxes, gt_bboxes)\n            val_losses.append(val_loss.item())\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}\")\n\n    scheduler.step(avg_val_loss)  # --- STEP THE SCHEDULER ---\n\n    # Early stopping check\n    if avg_val_loss < best_val_loss - 1e-4:  # minimal improvement threshold\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        best_model_wts = copy.deepcopy(model.state_dict())\n        print(f\"Validation loss improved, saving best model at epoch {epoch+1}.\")\n    else:\n        epochs_no_improve += 1\n        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n# Load best model weights after training\nmodel.load_state_dict(best_model_wts)\nprint(\"Training complete. Best model loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:47:16.447967Z","iopub.execute_input":"2025-08-15T16:47:16.448241Z","iopub.status.idle":"2025-08-15T16:53:29.910994Z","shell.execute_reply.started":"2025-08-15T16:47:16.448221Z","shell.execute_reply":"2025-08-15T16:53:29.910045Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 307/307 [00:40<00:00,  7.66it/s, avg_loss=3.28, batch_loss=2.85]","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Training Loss: 3.2849\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Validation Loss: 2.9773\nValidation loss improved, saving best model at epoch 1.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 307/307 [00:29<00:00, 10.49it/s, avg_loss=2.96, batch_loss=4.19]","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] Training Loss: 2.9591\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] Validation Loss: 2.9680\nValidation loss improved, saving best model at epoch 2.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 307/307 [00:29<00:00, 10.28it/s, avg_loss=3.02, batch_loss=3.53]","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] Training Loss: 3.0192\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] Validation Loss: 3.0682\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 307/307 [00:29<00:00, 10.30it/s, avg_loss=2.98, batch_loss=3.24]","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] Training Loss: 2.9777\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] Validation Loss: 3.0834\nNo improvement in validation loss for 2 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 307/307 [00:29<00:00, 10.40it/s, avg_loss=2.95, batch_loss=4.14]","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] Training Loss: 2.9549\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] Validation Loss: 2.9427\nValidation loss improved, saving best model at epoch 5.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 307/307 [00:29<00:00, 10.37it/s, avg_loss=2.92, batch_loss=2.32]","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] Training Loss: 2.9177\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] Validation Loss: 2.9586\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 307/307 [00:29<00:00, 10.38it/s, avg_loss=2.87, batch_loss=2.95]","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] Training Loss: 2.8749\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] Validation Loss: 2.8977\nValidation loss improved, saving best model at epoch 7.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 307/307 [00:29<00:00, 10.27it/s, avg_loss=2.98, batch_loss=2.7] ","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10] Training Loss: 2.9823\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10] Validation Loss: 3.0793\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 307/307 [00:29<00:00, 10.32it/s, avg_loss=2.91, batch_loss=1.83]","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10] Training Loss: 2.9064\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10] Validation Loss: 2.9199\nNo improvement in validation loss for 2 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 307/307 [00:30<00:00, 10.23it/s, avg_loss=2.9, batch_loss=2.75] ","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10] Training Loss: 2.8958\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10] Validation Loss: 2.9911\nNo improvement in validation loss for 3 epochs.\nEarly stopping triggered.\nTraining complete. Best model loaded.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Save the best model weights to disk\ntorch.save(model.state_dict(), \"best_model.pt\")\nprint(\"Saved best model weights to 'best_model.pt'\")\n# Load the best model weights from disk\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\nprint(\"Best model weights loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:53:39.446566Z","iopub.execute_input":"2025-08-15T16:53:39.446882Z","iopub.status.idle":"2025-08-15T16:53:40.683537Z","shell.execute_reply.started":"2025-08-15T16:53:39.446842Z","shell.execute_reply":"2025-08-15T16:53:40.682818Z"}},"outputs":[{"name":"stdout","text":"Saved best model weights to 'best_model.pt'\nBest model weights loaded.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def safe_unpack_orig_size(orig_size):\n    import numpy as np\n    if isinstance(orig_size, torch.Tensor):\n        # Convert tensor to numpy array and flatten\n        orig_size = orig_size.detach().cpu().numpy().flatten()\n    \n    # Check if the array has at least two elements\n    if len(orig_size) < 2:\n        return 0, 0\n    \n    return int(orig_size[0]), int(orig_size[1])\n\ndef denormalize_bbox(bbox, width, height):\n    import numpy as np\n    if isinstance(bbox, torch.Tensor):\n        bbox = bbox.detach().cpu().numpy()\n    bbox = np.array(bbox).flatten()[:4]\n    return [\n        float(bbox[0]) * width,\n        float(bbox[1]) * height,\n        float(bbox[2]) * width,\n        float(bbox[3]) * height\n    ]\n\ndef box_iou(box1, box2):\n    x1 = float(max(box1[0], box2[0]))\n    y1 = float(max(box1[1], box2[1]))\n    x2 = float(min(box1[2], box2[2]))\n    y2 = float(min(box1[3], box2[3]))\n    inter_w = max(0., x2 - x1)\n    inter_h = max(0., y2 - y1)\n    inter_area = inter_w * inter_h\n    box1_area = max(0., box1[2] - box1[0]) * max(0., box1[3] - box1[1])\n    box2_area = max(0., box2[2] - box2[0]) * max(0., box2[3] - box2[1])\n    union_area = box1_area + box2_area - inter_area + 1e-6\n    if union_area == 0.:\n        return 0.\n    return inter_area / union_area\n\ndef evaluate_model_iou(model, data_loader, device, max_batches=None):\n    model.eval()\n    all_ious = []\n    import numpy as np\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(data_loader):\n            clip_images = batch['clip_image'].to(device)\n            clip_texts = batch['clip_text'].to(device)\n            pred_bboxes = model(clip_images, clip_texts)\n            gt_bboxes = batch['bbox']\n            orig_sizes = batch['orig_size']\n            \n            # Use zip for safe iteration over batch elements\n            for pred_box_norm, gt_box_norm, orig_size_item in zip(pred_bboxes, gt_bboxes, orig_sizes):\n                width, height = safe_unpack_orig_size(orig_size_item)\n                \n                # Denormalize bounding boxes\n                pred_box_list = denormalize_bbox(pred_box_norm, width, height)\n                gt_box_list = denormalize_bbox(gt_box_norm, width, height)\n                \n                # Calculate IoU and append\n                iou = box_iou(pred_box_list, gt_box_list)\n                all_ious.append(iou)\n\n            if max_batches is not None and batch_idx + 1 >= max_batches:\n                break\n    \n    mean_iou = np.mean(all_ious) if all_ious else 0.0\n    print(f\"Mean IoU: {mean_iou:.4f} on {len(all_ious)} samples\")\n    return mean_iou\n\n# Example usage to demonstrate a successful run:\n# # Load best model weights before evaluation\n# model.load_state_dict(torch.load(\"best_model.pt\"))\n# model.eval()\n#\n# # Evaluate on validation set\n# val_mean_iou = evaluate_model_iou(model, val_loader, device)\n# print(\"Validation Mean IoU:\", val_mean_iou)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:53:44.463777Z","iopub.execute_input":"2025-08-15T16:53:44.464463Z","iopub.status.idle":"2025-08-15T16:53:44.475255Z","shell.execute_reply.started":"2025-08-15T16:53:44.464438Z","shell.execute_reply":"2025-08-15T16:53:44.474637Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Load best model weights before evaluation\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\n# Evaluate on validation set\nval_mean_iou = evaluate_model_iou(model, val_loader, device)\nprint(\"Validation Mean IoU:\", val_mean_iou)\n\n# Evaluate on test set\ntest_mean_iou = evaluate_model_iou(model, test_loader, device)\nprint(\"Test Mean IoU:\", test_mean_iou)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:53:49.892531Z","iopub.execute_input":"2025-08-15T16:53:49.893032Z","iopub.status.idle":"2025-08-15T16:54:05.427839Z","shell.execute_reply.started":"2025-08-15T16:53:49.893007Z","shell.execute_reply":"2025-08-15T16:54:05.427047Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.2896 on 132 samples\nValidation Mean IoU: 0.289642835112249\nMean IoU: 0.2981 on 132 samples\nTest Mean IoU: 0.2981048999688267\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import torch\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport clip\n\ndef visualize_prediction_and_crop(model, clip_preprocess, device, image_path, text_query):\n    \"\"\"\n    Takes a model, image path, and text query, and returns the cropped image\n    along with a visualization of the predicted bounding box.\n\n    Args:\n        model: The trained CrossModalClipFusionModel.\n        clip_preprocess: The CLIP image preprocessing function.\n        device: The device to run the model on ('cuda' or 'cpu').\n        image_path (str): The path to the input image.\n        text_query (str): The text description of the object to find.\n\n    Returns:\n        image_with_box (PIL.Image): The original image with the bounding box drawn.\n        cropped_image (PIL.Image): The cropped region of the image.\n    \"\"\"\n    # 1. Load and preprocess the image and text\n    image = Image.open(image_path).convert('RGB')\n    width, height = image.size\n    \n    clip_image = clip_preprocess(image).unsqueeze(0).to(device)\n    clip_text = clip.tokenize([text_query], truncate=True).to(device)\n\n    # 2. Get the model's bounding box prediction\n    model.eval()\n    with torch.no_grad():\n        pred_bbox_norm = model(clip_image, clip_text)\n    \n    # 3. Denormalize the predicted bounding box\n    x_min_norm, y_min_norm, x_max_norm, y_max_norm = pred_bbox_norm[0].cpu().numpy()\n    \n    x_min = int(x_min_norm * width)\n    y_min = int(y_min_norm * height)\n    x_max = int(x_max_norm * width)\n    y_max = int(y_max_norm * height)\n    \n    # Ensure coordinates are within image bounds\n    x_min = max(0, x_min)\n    y_min = max(0, y_min)\n    x_max = min(width, x_max)\n    y_max = min(height, y_max)\n\n    print(\"Predicted Bounding Box (x_min, y_min, x_max, y_max):\", (x_min, y_min, x_max, y_max))\n\n    # 4. Create a copy to draw on and the cropped image\n    image_with_box = image.copy()\n    draw = ImageDraw.Draw(image_with_box)\n    draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n    \n    # 5. Crop the image based on the predicted bounding box\n    cropped_image = image.crop((x_min, y_min, x_max, y_max))\n\n    return image_with_box, cropped_image\n\n# Assume 'model', 'clip_preprocess', and 'device' are already defined from previous steps\n# The following code block will run the test on the specified image.\n\n# Image path and text query\nimage_path = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014/COCO_test2014_000000000016.jpg\"\ntext_query = \"a baseball player swinging a bat\"\n\n# Run the function\ntry:\n    image_with_box, cropped_region = visualize_prediction_and_crop(\n        model=model,\n        clip_preprocess=test_dataset.clip_preprocess,\n        device=device,\n        image_path=image_path,\n        text_query=text_query\n    )\n    \n    # Display the results\n    image_with_box.save(\"prediction_visualization.png\")\n    cropped_region.save(\"cropped_region.png\")\n    print(\"Saved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:54:38.760165Z","iopub.execute_input":"2025-08-15T16:54:38.760455Z","iopub.status.idle":"2025-08-15T16:54:38.949334Z","shell.execute_reply.started":"2025-08-15T16:54:38.760433Z","shell.execute_reply":"2025-08-15T16:54:38.948425Z"}},"outputs":[{"name":"stdout","text":"Predicted Bounding Box (x_min, y_min, x_max, y_max): (30, 97, 479, 639)\nSaved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import torch\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport clip\n\ndef visualize_prediction_and_crop(model, clip_preprocess, device, image_path, text_query):\n    \"\"\"\n    Takes a model, image path, and text query, and returns the cropped image\n    along with a visualization of the predicted bounding box.\n\n    Args:\n        model: The trained CrossModalClipFusionModel.\n        clip_preprocess: The CLIP image preprocessing function.\n        device: The device to run the model on ('cuda' or 'cpu').\n        image_path (str): The path to the input image.\n        text_query (str): The text description of the object to find.\n\n    Returns:\n        image_with_box (PIL.Image): The original image with the bounding box drawn.\n        cropped_image (PIL.Image): The cropped region of the image.\n    \"\"\"\n    # 1. Load and preprocess the image and text\n    image = Image.open(image_path).convert('RGB')\n    width, height = image.size\n    \n    clip_image = clip_preprocess(image).unsqueeze(0).to(device)\n    clip_text = clip.tokenize([text_query], truncate=True).to(device)\n\n    # 2. Get the model's bounding box prediction\n    model.eval()\n    with torch.no_grad():\n        pred_bbox_norm = model(clip_image, clip_text)\n    \n    # 3. Denormalize the predicted bounding box\n    x_min_norm, y_min_norm, x_max_norm, y_max_norm = pred_bbox_norm[0].cpu().numpy()\n    \n    x_min = int(x_min_norm * width)\n    y_min = int(y_min_norm * height)\n    x_max = int(x_max_norm * width)\n    y_max = int(y_max_norm * height)\n    \n    # Ensure coordinates are within image bounds\n    x_min = max(0, x_min)\n    y_min = max(0, y_min)\n    x_max = min(width, x_max)\n    y_max = min(height, y_max)\n\n    print(\"Predicted Bounding Box (x_min, y_min, x_max, y_max):\", (x_min, y_min, x_max, y_max))\n\n    # 4. Create a copy to draw on and the cropped image\n    image_with_box = image.copy()\n    draw = ImageDraw.Draw(image_with_box)\n    draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n    \n    # 5. Crop the image based on the predicted bounding box\n    cropped_image = image.crop((x_min, y_min, x_max, y_max))\n\n    return image_with_box, cropped_image\n\n# Assume 'model', 'clip_preprocess', and 'device' are already defined from previous steps\n# The following code block will run the test on the specified image.\n\n# Image path and text query\nimage_path = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014/COCO_test2014_000000000069.jpg\"\ntext_query = \"man pouring wine\"\n\n# Run the function\ntry:\n    image_with_box, cropped_region = visualize_prediction_and_crop(\n        model=model,\n        clip_preprocess=test_dataset.clip_preprocess,\n        device=device,\n        image_path=image_path,\n        text_query=text_query\n    )\n    \n    # Display the results\n    image_with_box.save(\"prediction_visualization.png\")\n    cropped_region.save(\"cropped_region.png\")\n    print(\"Saved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T16:54:59.879318Z","iopub.execute_input":"2025-08-15T16:54:59.879603Z","iopub.status.idle":"2025-08-15T16:55:00.106376Z","shell.execute_reply.started":"2025-08-15T16:54:59.879580Z","shell.execute_reply":"2025-08-15T16:55:00.105802Z"}},"outputs":[{"name":"stdout","text":"Predicted Bounding Box (x_min, y_min, x_max, y_max): (21, 28, 639, 428)\nSaved visualization to 'prediction_visualization.png' and cropped region to 'cropped_region.png'\n","output_type":"stream"}],"execution_count":32}]}